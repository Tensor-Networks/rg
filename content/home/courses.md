---
widget: blank
headless: true
weight: 30
# title: Explore talks
# subtitle:
# content:
#   filters:
#     folders:
#       - course
#     kinds:
#       - section
#     exclude_tags:
#       - preface

  # filter_default: 0

  # filter_button:
  #   - name: All Talks
  #     tag: '*'
  #   - name: Previous
  #     tag: previous
  #   - name: Current
  #     tag: current
# design:
  # columns: '1'
#   view: masonry
#   flip_alt_rows: false
---

  ---
<section id="courses" class="some-section">
    <div class="row">
        <div class="col-12">
            <div class="listing" style="clear:both;">
                <div class="left" style="margin-bottom: 10px;">
                    <h3 style="text-align:center; font-style: italic;"> Talks</h3>
                    <!-- Add a new talk -->
                    <h4 style="margin-top: 20px;">Oct 17, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Introduction to Tensor Networks </a> 
                            </b>
                            <br> Presenter: Guillaume Rabusseau  
                            <br>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Guillaume Rabusseau is an assistant professor at Univeristé de Montréal since 2018 and holds a Canada CIFAR AI chair at the Mila research institute since 2019. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra, and on the tensor network formalism.
                                </div>
                            </div>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract">
                                <div class="card card-body" style="margin-top: 10px;">
                                    In this talk, I will give a tutorial on tensor networks and briefly present some of their applications in Machine Learning.
                                    Tensors are high order generalization of vectors and matrices. Similar to matrix factorization techniques, one of the goal of tensor decomposition techniques is to express a tensor as a product of small factors, thus reducing the number of parameters and potentially regularizing machine learning models. While linear algebra is ubiquitous and taught in most undergrad curriculum, tensor and multilinear algebra can be daunting. In this talk, I will try to give an easy and accessible introduction to tensor methods using the tensor network formalism. Tensor networks are an intuitive diagrammatic notation allowing one to easily reason about complex operations on high-order tensors.
                                </div>
                            </div>
                            <a href="notes/TNRG.pdf" download="TNRG.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                    <!-- Add more talks here if needed -->
                </div>
            </div>
        </div>
    </div>
                    <!-- Add a new talk -->
                    <h4 style="margin-top: 20px;">Oct 24, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Tensor network structure search (TN-SS) </a> 
                            </b>
                            <br> Presenter: Chao Li  
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                             <a href="https://arxiv.org/abs/2304.12875" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading
                             </a>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_0" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                               Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_0">
                                <div class="card card-body" style="margin-top: 10px;">
                                 Tensor networks (TN) represent a formidable framework within machine learning. However, the selection of an effective TN model—a process known as Tensor Network Structure Search (TN-SS)—remains a computationally challenging endeavor. In my presentation, I will offer a succinct overview of our approach to this issue. I will concentrate on problem formulation and solution strategies from the standpoint of discrete optimization. Specifically, I will discuss three algorithms and the associated theoretical findings, which have been the subject of my research and were published in ICML conferences in 2020, 2022, and 2023.
                                </div>
                            </div>
                            <a href="notes/TNSSMILA.pdf" download="TNSSMILA.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                    <!-- Add more talks here if needed -->
                    <h4 style="margin-top: 20px;">Oct 31, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: TNS and non-equilibrium dynamics </a> 
                            </b>
                            <br> Presenter: Mari Carmen Bañuls 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_3" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_3">
                                <div class="card card-body" style="margin-top: 10px;">
                                Tensor Network States (TNS) are ansatzes that can efficiently represent certain states of quantum many-body systems. In one spatial dimension, the paradigmatic example is the family of Matrix Product States (MPS), extremely powerful to study ground states, low energy excitations, and thermal equilibrium states. Quantum information theory provides tools to understand why TNS are good ansatzes for physically relevant states, and some of the limitations connected to the simulation algorithms. Most significantly, while to some extent TNS can be used to study real time evolution, a full description of the most general out-of-equilibrium setup is often out of reach.  
In this talk I will present the basic ideas behind TNS algorithms, as well as the limitations and some alternative approaches that try to push their application for dynamical problems.
                                </div>
                            </div>
                            <!-- <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a> -->
                            <a href="https://arxiv.org/abs/2209.11788" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                        </li>
                    </ul>
                     <!-- Add more talks here if needed -->
                    <h4 style="margin-top: 20px;">Nov 7th, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: MultiHU-TD: Multi-feature Hyperspectral Unmixing Based on Tensor Decomposition </a> 
                            </b>
                            <br> Presenter: Mohamad Jouni 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <!-- <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a> -->
                            <a href="https://arxiv.org/abs/2310.03860" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Nov 14, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement </a> 
                            </b>
                            <br> Presenter: Yotam Alexander 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <!-- <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a> -->
                            <a href="https://arxiv.org/abs/2303.11249" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Nov 21, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: High-dimensional density estimation with tensorizing flow </a> 
                            </b>
                            <br> Presenter: Yinuo Ren 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <!-- <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a> -->
                            <a href="https://arxiv.org/abs/2212.00759" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Dec 4, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Enhancing Generative Models via Quantum Correlations </a> 
                            </b>
                            <br> Presenter: Eric R. Anschuetz 
                            <br>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio_dec4" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio_dec4">
                                <div class="card card-body" style="margin-top: 10px;">
Eric is a Sherman Fairchild postdoctoral fellow at Caltech, having recently finished his PhD under Aram Harrow at MIT. His research focuses on studying the performance of quantum machine learning algorithms: when they are useful, when they are feasible, and when they break. Previously, Eric has shown how techniques from random matrix theory and Morse theory can be used to study the loss landscapes of quantum machine learning models. Most recently, he has shown how nonuniversal quantum resources can lead to expressivity advantages over classical generative models using techniques from quantum foundations research.
                                </div>
                            </div> 
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_dec4" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_dec4">
                                <div class="card card-body" style="margin-top: 10px;">
                                Quantum mechanical systems can produce probability distributions that exhibit quantum correlations which are difficult to capture using classical models. We show theoretically that such quantum correlations provide a powerful resource for generative modeling. In particular, we provide an unconditional proof of separation in expressive power between a class of widely used generative models—known as Bayesian networks—and its minimal quantum extension, which can be expressed as a class of matrix product states. We show that this expressivity enhancement is associated with quantum nonlocality and quantum contextuality. The possibility of quantum enhancement demonstrated in this work not only sheds light on the design of useful quantum machine-learning protocols but also provides inspiration to draw on ideas from quantum foundations to improve purely classical algorithms. We also briefly discuss more recent results demonstrating a similar separation between classical sequence models (including encoder-decoder models such as Transformers) and a class of quantized recurrent neural networks.
                                </div>
                            </div>
                             <!-- <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a> -->
                            <a href="https://arxiv.org/abs/2101.08354" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</section>

---

