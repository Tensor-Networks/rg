<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.6.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="Tensor Network Reading Group" />

  
  <link rel="alternate" hreflang="en-us" href="http://tensor-networks.github.io/" />

  
  
  
    <meta name="theme-color" content="#0072ff" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
      
      
        
      
        
      
        
      
        
      
        
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.85376cac46aadc8bb05ccd5d4cd02937.css" />

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



  


  


  




  
  
  
    <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
  

  
  
    <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Tensor Network Reading Group" />
  

  
  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  
  <link rel="icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="http://tensor-networks.github.io/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@TN" />
    <meta property="twitter:creator" content="@TN" />
  
  <meta property="og:site_name" content="Tensor Network Reading Group" />
  <meta property="og:url" content="http://tensor-networks.github.io/" />
  <meta property="og:title" content="Tensor Network Reading Group" />
  <meta property="og:description" content="Tensor Network Reading Group" /><meta property="og:image" content="http://tensor-networks.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="http://tensor-networks.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
  

  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite","url": "http://tensor-networks.github.io"
}
</script>


  
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "RG",
  "@id": "http://tensor-networks.github.io",
  "name": "Tensor Network Reading Group",
  "logo": "http://tensor-networks.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_3.png",
  
  
  
  
  "url": "http://tensor-networks.github.io"
}
</script>

  

  


  <title>Tensor Network Reading Group</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper   "  >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.1ee5462d74c6c0de1f8881b384ecc58d.js"></script>

  




  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Tensor Network Reading Group</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Tensor Network Reading Group</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          

          

          
          
          
          

          
            
          

          <li class="nav-item">
            <a class="nav-link  active" href="/"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#courses" data-target="#courses"><span>Schedule</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#people" data-target="#people"><span>Organizers</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        

        
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    


  









  
<span class="js-widget-page d-none"></span>





  
  
  
  





  

  
  
  
  

  

  
    
    
  

  
  
  
  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="hero" class="home-section wg-hero light "  >
   <div class="home-section-bg " style="background-image: linear-gradient(0deg, rgb(255,255,255), rgb(255,255,255));">
     
   </div>
    <div class="container">

    

      




<div class="row">
  <div class="col-12 col-md-6 text-center text-md-left">


    

    
      <div class="hero-lead"><br>
<!-- Published with the [Wowchemy Website Builder](https://wowchemy.com/) for Hugo. -->
<p>Every Tuesday 11:30am to 12:30pm EST. <br><p style="font-size: 20px;">Join us on zoom  <a href="https://umontreal.zoom.us/j/83763780264?pwd=UlN1WVkrZWk3ZVJZZ3RRMkZ3RUdQdz09">here</a> <br>Sign up <a href="https://docs.google.com/forms/d/e/1FAIpQLSdCsxq2W1hdMgn5PCbEFu8yCVoPuxuCOFMbE5Q-5pIS7wd4Eg/viewform?usp=sf_link">here</a> to receive email communications <br>Visit our <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA"> Youtube channel</a> for the recordings<br> Join the <a href="https://join.slack.com/t/tenread/shared_invite/zt-23sjql6d3-Cni6mcr2QEmX1rPRUZVYlg"> TeNRead Slack channel </a> for discussions</p></p>
<!-- <a class="github-button" href="https://github.com/wowchemy/wowchemy-hugo-themes" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star Wowchemy Website Builder for Hugo">Star Wowchemy Website Builder for Hugo</a><br><a class="github-button" href="https://github.com/wowchemy/starter-hugo-online-course" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star the Online Course template">Star the Online Course template</a><script async defer src="https://buttons.github.io/buttons.js"></script> -->
</div>
    

    
    

    
    

  
  
  </div>
  <div class="col-12 mx-auto col-md-6  hero-media">
      
      

        
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/media/teacher_hu0d4286698ae168fc036c4199988012de_26588_400x0_resize_lanczos_3.png" srcset="/media/teacher_hu0d4286698ae168fc036c4199988012de_26588_1200x0_resize_lanczos_3.png 1200w,/media/teacher_hu0d4286698ae168fc036c4199988012de_26588_800x0_resize_lanczos_3.png 800w,/media/teacher_hu0d4286698ae168fc036c4199988012de_26588_400x0_resize_lanczos_3.png 400w" width="283" height="212" alt="">
      
    
  </div>
</div>



    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="courses" class="home-section wg-blank  "  >
   <div class="home-section-bg " >
     
   </div>
    <div class="container">

    
      <div class="row  justify-content-center">
      
    

      



  <div class="col-12 col-lg-8">
    <hr>
<section id="courses" class="some-section">
    <div class="row">
        <div class="col-12">
            <div class="listing" style="clear:both;">
                <div class="left" style="margin-bottom: 10px;">
                    <h3 style="text-align:center; font-style: italic;"> Upcoming Talks, Winter 2024</h3>
                     <h4 style="margin-top: 20px;">Apr 9, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Multilinear Operator Networks </a> 
                            </b>
                            <br> Presenter: Yixin Cheng 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> -->
                            <!-- <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Guillaume Rabusseau is an assistant professor at Univeristé de Montréal since 2018 and holds a Canada CIFAR AI chair at the Mila research institute since 2019. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra, and on the tensor network formalism.
                                </div>
                            </div> -->
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_apr9" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_apr9">
                                <div class="card card-body" style="margin-top: 10px;">    
                                Despite the remarkable capabilities of deep neural networks in image recognition,
                                the dependence on activation functions remains a largely unexplored area and has
                                yet to be eliminated. On the other hand, Polynomial Networks is a class of models
                                that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which
                                relies solely on multilinear operators. The core layer of MONet, called Mu-Layer,
                                captures multiplicative interactions of the elements of the input token. MONet
                                captures high-degree interactions of the input elements and we demonstrate the
                                efficacy of our approach on a series of image recognition and scientific computing
                                benchmarks. The proposed model outperforms prior polynomial networks and
                                performs on par with modern architectures. We believe that MONet can inspire
                                further research on models that use entirely multilinear operations.
                                </div>
                            </div>
                            <!-- <a href="notes/talk_DEMOTE.pdf" download="talk_DEMOTE.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a> -->
                            <a href="https://arxiv.org/pdf/2401.17992.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                            <!-- <a href="https://youtu.be/Ksj2kB6RSk0" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a> -->
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</section>
<hr>
<section id="courses" class="some-section">
    <div class="row">
        <div class="col-12">
            <div class="listing" style="clear:both;">
                <div class="left" style="margin-bottom: 10px;">
                    <h3 style="text-align:center; font-style: italic;"> Past Talks, Winter 2024</h3>
                    <!-- Add more talks here if needed -->
                     <!-- Add more talks here if needed -->
                     <!-- Add a new talk -->
                    <h4 style="margin-top: 20px;">Apr 9, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes </a> 
                            </b>
                            <br> Presenter: Shikai Fang 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> -->
                            <!-- <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Guillaume Rabusseau is an assistant professor at Univeristé de Montréal since 2018 and holds a Canada CIFAR AI chair at the Mila research institute since 2019. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra, and on the tensor network formalism.
                                </div>
                            </div> -->
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_apr9" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_apr9">
                                <div class="card card-body" style="margin-top: 10px;">    
                                Tensor decomposition is an important tool for multiway data analysis. In practice,
                                the data is often sparse yet associated with rich temporal information. Existing
                                methods, however, often under-use the time information and ignore the structural
                                knowledge within the sparsely observed tensor entries. To overcome these limitations and to better capture the underlying temporal structure, we propose Dynamic
                                EMbedIngs fOr dynamic Tensor dEcomposition (DEMOTE). We develop a neural
                                diffusion-reaction process to estimate dynamic embeddings for the entities in each
                                tensor mode. Specifically, based on the observed tensor entries, we build a multipartite graph to encode the correlation between the entities. We construct a graph
                                diffusion process to co-evolve the embedding trajectories of the correlated entities
                                and use a neural network to construct a reaction process for each individual entity.
                                In this way, our model can capture both the commonalities and personalities during
                                the evolution of the embeddings for different entities. We then use a neural network
                                to model the entry value as a nonlinear function of the embedding trajectories. For
                                model estimation, we combine ODE solvers to develop a stochastic mini-batch
                                learning algorithm. We propose a stratified sampling method to balance the cost of
                                processing each mini-batch so as to improve the overall efficiency. We show the advantage of our approach in both simulation study and real-world applications.
                                </div>
                            </div>
                            <a href="notes/talk_DEMOTE.pdf" download="talk_DEMOTE.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://youtu.be/Ksj2kB6RSk0" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                       <h4 style="margin-top: 20px;">Apr 2, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Equivariant Polynomials for Graph Neural Networks </a> 
                            </b>
                            <br> Presenter: Bobak Kiani 
                            <br>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio_apr2" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> 
                             <div class="collapse" id="newTalkBio_apr2">
                                <div class="card card-body" style="margin-top: 10px;">
                                    I am a postdoc at Harvard University in the Applied Mathematics and Computer Science department. Before this, I was a PhD student at MIT studying Electrical Engineering and Computer Science. My research areas are related to machine learning and quantum computation. In quantum computation, my work focuses on learning the limits and capabilities of algorithms especially those related to quantum machine learning. On the classical side, I am interested in theoretically studying the role of geometry and symmetries in learning algorithms.
                                </div>
                            </div> 
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_apr2" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_apr2">
                                <div class="card card-body" style="margin-top: 10px;">    
                                I will discuss applications of invariant and equivariant polynomials in learning on graph data. Our work presents an alternative expressiveness hierarchy for graph neural networks (GNNs) based on the ability of GNNs to calculate equivariant polynomials of a certain degree. I will present a basis for any graph equivariant polynomial and show how this basis can enhance expressivity and performance of GNN models. This work is in collaboration with colleagues at Meta AI, Weizmann, and MIT.
                                </div>
                            </div>
                            <!-- <a href="notes/TNRG.pdf" download="TNRG.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a> -->
                            <a href="https://youtu.be/HYfSGRLVdOk" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Mar 26, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Scalable symmetric Tucker tensor decomposition </a> 
                            </b>
                            <br> Presenter: Ruhui Jin 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> -->
                            <!-- <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Guillaume Rabusseau is an assistant professor at Univeristé de Montréal since 2018 and holds a Canada CIFAR AI chair at the Mila research institute since 2019. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra, and on the tensor network formalism.
                                </div>
                            </div> -->
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_mar26" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_mar26">
                                <div class="card card-body" style="margin-top: 10px;">    
                               We study the best low-rank Tucker decomposition of symmetric tensors. The motivating application
                                is decomposing higher-order multivariate moments. Moment tensors have special structure and are
                                important to various data science problems. We advocate for projected gradient descent (PGD) method
                                and higher-order eigenvalue decomposition (HOEVD) approximation as computation schemes. Most
                                importantly, we develop scalable adaptations of the basic PGD and HOEVD methods to decompose
                                sample moment tensors. With the help of implicit and streaming techniques, we evade the overhead cost
                                of building and storing the moment tensor. Such reductions make computing the Tucker decomposition
                                realizable for large data instances in high dimensions. Numerical experiments demonstrate the efficiency
                                of the algorithms and the applicability of moment tensor decompositions to real-world datasets. Finally
                                we study the convergence on the Grassmannian manifold, and prove that the update sequence derived
                                by the PGD solver achieves first- and second-order criticality
                                </div>
                            </div>
                            <!-- <a href="notes/TNRG.pdf" download="TNRG.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a> -->
                            <a href="https://youtu.be/MKub4Df7zCI" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                     <h4 style="margin-top: 20px;">Mar 19, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: When randomized algorithms meet tensor decompositions </a> 
                            </b>
                            <br> Presenter: Beheshteh T. Rakhshan 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> -->
                            <!-- <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Guillaume Rabusseau is an assistant professor at Univeristé de Montréal since 2018 and holds a Canada CIFAR AI chair at the Mila research institute since 2019. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra, and on the tensor network formalism.
                                </div>
                            </div> -->
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_mar19" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_mar19">
                                <div class="card card-body" style="margin-top: 10px;">    
                            Tensor decomposition methods have recently found numerous applications in machine learning. Their ability to perform operations efficiently on very high-dimensional tensor data makes them ubiquitous in data science and machine learning. Due to the curse of dimensionality of tensors, designing efficient algorithms for very high dimensional tensor data is challenging. On the other hand, finding a decomposition of high-dimensional tensors is crucial. Many tensor decompositions correspond to either difficult non-convex optimization problems or the running time is exponential in the order of a tensor. To address these issues, several recent works have developed randomized-based algorithms. These issues have led to the search for alternatives based on randomization and sampling techniques. In this talk, we will be giving a tutorial on randomized algorithms and their application in tensor network methods.
                                </div>
                            </div>
                            <a href="notes/Note Feb 11, 2024 (3).pdf" download="Note Feb 11, 2024 (3).pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://youtu.be/_ArtT2W8SHY" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                     <h4 style="margin-top: 20px;">Mar 12, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Approximately Optimal Core Shapes for Tensor Decompositions </a> 
                            </b>
                            <br> Presenter: Mehrdad Ghadiri 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> -->
                            <!-- <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Guillaume Rabusseau is an assistant professor at Univeristé de Montréal since 2018 and holds a Canada CIFAR AI chair at the Mila research institute since 2019. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra, and on the tensor network formalism.
                                </div>
                            </div> -->
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_mar12" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_mar12">
                                <div class="card card-body" style="margin-top: 10px;">    
                                This work studies the combinatorial optimization
                                problem of finding an optimal core tensor shape,
                                also called multilinear rank, for a size-constrained
                                Tucker decomposition. We give an algorithm with
                                provable approximation guarantees for its reconstruction error via connections to higher-order singular values. Specifically, we introduce a novel
                                Tucker packing problem, which we prove is NPhard, and give a polynomial-time approximation
                                scheme based on a reduction to the 2-dimensional
                                knapsack problem with a matroid constraint. We
                                also generalize our techniques to tree tensor network decompositions. We implement our algorithm using an integer programming solver, and
                                show that its solution quality is competitive with
                                (and sometimes better than) the greedy algorithm
                                that uses the true Tucker decomposition loss at
                                each step, while also running up to 1000x faster
                                </div>
                            </div>
                            <!-- <a href="notes/TNRG.pdf" download="TNRG.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a> -->
                            <a href="https://youtu.be/K63Al4qEaVA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                     <h4 style="margin-top: 20px;">Mar 5, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: SketchySGD: Reliable Stochastic Optimization via Randomized Curvature Estimates </a> 
                            </b>
                            <br> Presenter: Zachary Joseph Frangella 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> -->
                            <!-- <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Guillaume Rabusseau is an assistant professor at Univeristé de Montréal since 2018 and holds a Canada CIFAR AI chair at the Mila research institute since 2019. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra, and on the tensor network formalism.
                                </div>
                            </div> -->
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_mar5" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_mar5">
                                <div class="card card-body" style="margin-top: 10px;">  
                                We introduce SketchySGD, a stochastic quasi-Newton method that uses sketching to approximate
                                the curvature of the loss function. SketchySGD improves upon existing stochastic gradient methods
                                in machine learning by using randomized low-rank approximations to the subsampled Hessian and
                                by introducing an automated stepsize that works well across a wide range of convex machine learning
                                problems. We show theoretically that SketchySGD with a fixed stepsize converges linearly to a small
                                ball around the optimum. Further, in the ill-conditioned setting we show SketchySGD converges
                                at a faster rate than SGD for least-squares problems. We validate this improvement empirically
                                with ridge regression experiments on real data. Numerical experiments on both ridge and logistic
                                regression problems with dense and sparse data, show that SketchySGD equipped with its default
                                hyperparameters can achieve comparable or better results than popular stochastic gradient methods,
                                even when they have been tuned to yield their best performance. In particular, SketchySGD is able
                                to solve an ill-conditioned logistic regression problem with a data matrix that takes more than
                                840GB RAM to store, while its competitors, even when tuned, are unable to make any progress.
                                SketchySGD’s ability to work out-of-the box with its default hyperparameters and excel on illconditioned problems is an advantage over other stochastic gradient methods, most of which require
                                careful hyperparameter tuning (especially of the learning rate) to obtain good performance and
                                degrade in the presence of ill-conditioning.  
                                </div>
                            </div>
                            <!-- <a href="notes/TNRG.pdf" download="TNRG.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a> -->
                            <a href="https://youtu.be/-cyvLRZ1XzE" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Feb 27, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: HyperAttention: Long-context Attention in Near-Linear Time</a> 
                            </b>
                            <br> Presenter: Amir Zandieh  
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> -->
                            <!-- <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Guillaume Rabusseau is an assistant professor at Univeristé de Montréal since 2018 and holds a Canada CIFAR AI chair at the Mila research institute since 2019. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra, and on the tensor network formalism.
                                </div>
                            </div> -->
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_feb27" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_feb27">
                                <div class="card card-body" style="margin-top: 10px;">    
                                We present an approximate attention mechanism named “HyperAttention” to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is
                                necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions
                                like FlashAttention. We validate the empirical performance HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50% faster on 32k context length while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer.
                                </div>
                            </div>
                            <!-- <a href="notes/TNRG.pdf" download="TNRG.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a> -->
                            <a href="https://youtu.be/QnO7ixLuwuc?si=THUoLeXTJ-PFqVHk" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Feb 20, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Cost-efficient Gaussian tensor network embeddings for tensor-structured inputs </a> 
                            </b>
                            <br> Presenter: Linjian Ma  
                            <br>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio_linjan" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio_linjan">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Linjian Ma is currently a research scientist at Meta Platforms. Before this role, he pursued his PhD in Computer Science at University of Illinois Urbana-Champaign, advised by Edgar Solomonik. His research interests are numerical algorithms and high-performance computing. His PhD thesis focused on developing efficient systems and numerical algorithms for tensor computations with applications in data analytics and quantum simulation.
                                </div>
                            </div>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_feb20" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_feb20">
                                <div class="card card-body" style="margin-top: 10px;">    
                               We propose novel sketching algorithms for both tensor decompositions and tensor networks. Sketching involves employing random matrices, also known as embeddings, to project data onto low-dimensional spaces, thereby reducing the computational cost of subsequent operations. In the context of data with a tensor network structure, we present efficient algorithms that utilize tensor network-structured embeddings to sketch the data. Moreover, we provide theoretical bounds on the accuracy of sketching achieved through these algorithms. The proposed sketching techniques are used to accelerate various problems involving tensor networks, including CP and Tucker tensor decompositions and tensor train rounding.
                                </div>
                            </div>
                            <a href="notes/pre_linjian_ma.pdf" download="pre_linjian_ma.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://youtu.be/YkfSZ26Opzw" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                    <!-- Add more talks here if needed -->
                    <h4 style="margin-top: 20px;">Feb 6, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Fast Exact Leverage Score Sampling from Khatri-Rao Products with Applications to Tensor Decomposition </a> 
                            </b>
                            <br> Presenter: Vivek Bharadwaj  
                            <br>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio_feb6" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> 
                            <div class="collapse" id="newTalkBio_feb6">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Vivek Bharadwaj is a PhD student at the University of California, Berkeley advised by James Demmel and Aydın Buluç. His interests include high-performance computing, randomized algorithms, and tensor decompositions at scale. He is affiliated with the BeBOP and PASSION groups at the university and its affiliate, Lawrence Berkeley National Laboratory.
                                </div>
                            </div>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_022024" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_022024">
                                <div class="card card-body" style="margin-top: 10px;">    
                              We present a data structure to randomly sample rows from the Khatri-Rao product
                                of several matrices according to the exact distribution of its leverage scores. Our
                                proposed sampler draws each row in time logarithmic in the height of the KhatriRao product and quadratic in its column count, with persistent space overhead
                                at most the size of the input matrices. As a result, it tractably draws samples
                                even when the matrices forming the Khatri-Rao product have tens of millions
                                of rows each. When used to sketch the linear least squares problems arising in
                                CANDECOMP / PARAFAC tensor decomposition, our method achieves lower
                                asymptotic complexity per solve than recent state-of-the-art methods. Experiments
                                on billion-scale sparse tensors validate our claims, with our algorithm achieving
                                higher accuracy than competing methods as the decomposition rank grows.
                                </div>
                            </div> 
                            <a href="notes/MILA_Tensor_Network_Reading_Group.pdf" download="MILA_Tensor_Network_Reading_Group.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://youtu.be/u4X5DBRbrjc" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a> 
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Jan 30, 2024</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Sampling-Based Decomposition Algorithms for Arbitrary Tensor Networks </a> 
                            </b>
                            <br> Presenter: Osman Malik  
                            <br>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio_jan30" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a> 
                            <div class="collapse" id="newTalkBio_jan30">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Osman is the 2021 Alvarez Postdoctoral Fellow at Lawrence Berkeley National Laboratory where he is a member of the Scalable Solvers Group. He received his PhD in Applied Mathematics from University of Colorado Boulder where he was advised by Stephen Becker. His research is evolving around randomized algorithms, tensor networks methods, optimization and quantum computing.
                                </div>
                            </div>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_012024" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_012024">
                                <div class="card card-body" style="margin-top: 10px;">    
                               We show how to develop sampling-based alternating least squares (ALS) algorithms for
                                decomposition of tensors into any tensor network (TN) format. Provided the TN format
                                satisfies certain mild assumptions, resulting algorithms will have input sublinear per-iteration
                                cost. Unlike most previous works on sampling-based ALS methods for tensor decomposition,
                                the sampling in our framework is done according to the exact leverage score distribution of the
                                design matrices in the ALS subproblems. We implement and test two tensor decomposition
                                algorithms that use our sampling framework in a feature extraction experiment where we
                                compare them against a number of other decomposition algorithms.
                                </div>
                            </div>
                            <a href="notes/TN reading group - to share.pdf" download="TNRG.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://youtu.be/j7qSlahlHRk" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
<hr>
<section id="courses" class="some-section">
    <div class="row">
        <div class="col-12">
            <div class="listing" style="clear:both;">
                <div class="left" style="margin-bottom: 10px;">
                    <h3 style="text-align:center; font-style: italic;"> Past Talks, Fall 2023</h3>
                    <!-- Add a new talk -->
                    <h4 style="margin-top: 20px;">Oct 17, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Introduction to Tensor Networks </a> 
                            </b>
                            <br> Presenter: Guillaume Rabusseau  
                            <br>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                    Guillaume Rabusseau is an assistant professor at Univeristé de Montréal since 2018 and holds a Canada CIFAR AI chair at the Mila research institute since 2019. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra, and on the tensor network formalism.
                                </div>
                            </div>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract">
                                <div class="card card-body" style="margin-top: 10px;">
                                    In this talk, I will give a tutorial on tensor networks and briefly present some of their applications in Machine Learning.
                                    Tensors are high order generalization of vectors and matrices. Similar to matrix factorization techniques, one of the goal of tensor decomposition techniques is to express a tensor as a product of small factors, thus reducing the number of parameters and potentially regularizing machine learning models. While linear algebra is ubiquitous and taught in most undergrad curriculum, tensor and multilinear algebra can be daunting. In this talk, I will try to give an easy and accessible introduction to tensor methods using the tensor network formalism. Tensor networks are an intuitive diagrammatic notation allowing one to easily reason about complex operations on high-order tensors.
                                </div>
                            </div>
                            <a href="notes/TNRG.pdf" download="TNRG.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                    <!-- Add more talks here if needed -->
                </div>
            </div>
        </div>
    </div>
                    <!-- Add a new talk -->
                    <h4 style="margin-top: 20px;">Oct 24, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Tensor network structure search (TN-SS) </a> 
                            </b>
                            <br> Presenter: Chao Li  
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                             <a href="https://arxiv.org/abs/2304.12875" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading
                             </a>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_0" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                               Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_0">
                                <div class="card card-body" style="margin-top: 10px;">
                                 Tensor networks (TN) represent a formidable framework within machine learning. However, the selection of an effective TN model—a process known as Tensor Network Structure Search (TN-SS)—remains a computationally challenging endeavor. In my presentation, I will offer a succinct overview of our approach to this issue. I will concentrate on problem formulation and solution strategies from the standpoint of discrete optimization. Specifically, I will discuss three algorithms and the associated theoretical findings, which have been the subject of my research and were published in ICML conferences in 2020, 2022, and 2023.
                                </div>
                            </div>
                            <a href="notes/TNSSMILA.pdf" download="TNSSMILA.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://www.youtube.com/channel/UCajE0Tzf0r3qORFsQIemglA" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                    <!-- Add more talks here if needed -->
                    <h4 style="margin-top: 20px;">Oct 31, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: TNS and non-equilibrium dynamics </a> 
                            </b>
                            <br> Presenter: Mari Carmen Bañuls 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_3" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_3">
                                <div class="card card-body" style="margin-top: 10px;">
                                Tensor Network States (TNS) are ansatzes that can efficiently represent certain states of quantum many-body systems. In one spatial dimension, the paradigmatic example is the family of Matrix Product States (MPS), extremely powerful to study ground states, low energy excitations, and thermal equilibrium states. Quantum information theory provides tools to understand why TNS are good ansatzes for physically relevant states, and some of the limitations connected to the simulation algorithms. Most significantly, while to some extent TNS can be used to study real time evolution, a full description of the most general out-of-equilibrium setup is often out of reach.  
In this talk I will present the basic ideas behind TNS algorithms, as well as the limitations and some alternative approaches that try to push their application for dynamical problems.
                                </div>
                            </div>
                            <a href="notes/TNSandQMBDynamics_TNRG_Oct23.pdf" download="TNSandQMBDynamics_TNRG_Oct23.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://arxiv.org/abs/2209.11788" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                            <a href="https://www.youtube.com/watch?v=mrPAZ-ZqdGg" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                     <!-- Add more talks here if needed -->
                    <h4 style="margin-top: 20px;">Nov 7th, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: MultiHU-TD: Multi-feature Hyperspectral Unmixing Based on Tensor Decomposition </a> 
                            </b>
                            <br> Presenter: Mohamad Jouni 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <a href="notes/MultiHUTD_slides.pdf" download="MultiHUTD_slides.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://arxiv.org/abs/2310.03860" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                            <a href="https://youtu.be/OUimbKVZtvs?si=2KGShraYZD6PFxN7" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Nov 14, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement </a> 
                            </b>
                            <br> Presenter: Yotam Alexander 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <a href="notes/talk_slides_latest.pptx" download="talk_slides_latest.pptx" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://youtu.be/T7uY9cNu5Kw" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                            <a href="https://arxiv.org/abs/2303.11249" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Nov 21, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: High-dimensional density estimation with tensorizing flow </a> 
                            </b>
                            <br> Presenter: Yinuo Ren 
                            <br>
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <!-- <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract">
                                <div class="card card-body" style="margin-top: 10px;">
                                </div>
                            </div> -->
                            <a href="notes/slides.pdf" download="slides.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://youtu.be/qVW1_DnQXKI" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                            <a href="https://arxiv.org/abs/2212.00759" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Nov 28, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: AI and TN -  a love affair </a> 
                            </b>
                            <br> Presenters: Bram Vanhecke 
                            <br>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio">
                                <div class="card card-body" style="margin-top: 10px;">
                                I studied at KU Leuven where I made one publication on supergravity with A. Van Proeyen. Then did a PhD in the group of Frank Verstraete working on tensor networks, specifically PEPS methods, criticality and
                                frustrated systems. Now I'm doing a post-doc UniWien in the group of Norbert Schuch. Current work continues in tensor networks, but branching out into some machine learning applications with Samuel Wauthier and
                                continuous tensor networks (unpublished).
                                </div>
                            </div> 
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract">
                                <div class="card card-body" style="margin-top: 10px;">
                                Machine learning and tensor networks have grown closer these past few years as they both learn and borrow from one another. We will discuss one project where tensor networks are used to construct a generative
                                model for active inference planning (a theory in neuroscience that models human learning and cognition), hence using tensor networks to do human-inspired machine learning. And also another project where a technique developed for deep learning, automatic differentiation, is applied to a standard tensor network problem: PEPS optimization. The latter focusses on three major issues in applying AD in this new context, and presents efficient solutions.  One of these has also implications for AD users who aren't using it for PEPS.
                                </div>
                            </div> 
                            <a href="notes/presentation_Vienna.pdf" download="presentation_Vienna.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                            <a href="https://youtu.be/EaqdAWCO5p8" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                            <a href="https://arxiv.org/abs/2208.08713" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading 1</a>
                            <a href="https://arxiv.org/abs/2311.11894" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading 2</a>
                        </li>
                    </ul>
                    <h4 style="margin-top: 20px;">Dec 4, 2023</h4>
                    <ul style="padding-left: 0;">
                        <li style="list-style-type: none;">
                            <b>
                                <a style="color: #2c3e50;"> Title: Enhancing Generative Models via Quantum Correlations </a> 
                            </b>
                            <br> Presenter: Eric R. Anschuetz 
                            <br>
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkBio_dec4" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Speaker Bio
                            </a>
                            <div class="collapse" id="newTalkBio_dec4">
                                <div class="card card-body" style="margin-top: 10px;">
Eric is a Sherman Fairchild postdoctoral fellow at Caltech, having recently finished his PhD under Aram Harrow at MIT. His research focuses on studying the performance of quantum machine learning algorithms: when they are useful, when they are feasible, and when they break. Previously, Eric has shown how techniques from random matrix theory and Morse theory can be used to study the loss landscapes of quantum machine learning models. Most recently, he has shown how nonuniversal quantum resources can lead to expressivity advantages over classical generative models using techniques from quantum foundations research.
                                </div>
                            </div> 
                            <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#newTalkAbstract_dec4" role="button" aria-expanded="false" aria-controls="collapseExample" style="margin-top: 10px;">
                                Abstract
                            </a>
                            <div class="collapse" id="newTalkAbstract_dec4">
                                <div class="card card-body" style="margin-top: 10px;">
                                Quantum mechanical systems can produce probability distributions that exhibit quantum correlations which are difficult to capture using classical models. We show theoretically that such quantum correlations provide a powerful resource for generative modeling. In particular, we provide an unconditional proof of separation in expressive power between a class of widely used generative models—known as Bayesian networks—and its minimal quantum extension, which can be expressed as a class of matrix product states. We show that this expressivity enhancement is associated with quantum nonlocality and quantum contextuality. The possibility of quantum enhancement demonstrated in this work not only sheds light on the design of useful quantum machine-learning protocols but also provides inspiration to draw on ideas from quantum foundations to improve purely classical algorithms. We also briefly discuss more recent results demonstrating a similar separation between classical sequence models (including encoder-decoder models such as Transformers) and a class of quantized recurrent neural networks.
                                </div>
                            </div>
                             <a href="notes/Mila_Presentation_20231205.pdf" download="Mila_Presentation_20231205.pdf" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Notes</a>
                             <a href="https://youtu.be/rUbjqU98jJY?\si=qrin1jewP3au3P2C" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Recording Link</a>
                            <a href="https://arxiv.org/abs/2101.08354" class="btn btn-primary btn-xs" style="margin-top: 10px;"> Reading</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</section>
<hr>

  </div>



    
      </div>
    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="people" class="home-section wg-people  "  >
   <div class="home-section-bg " >
     
   </div>
    <div class="container">

    

      









<div class="row justify-content-center people-widget">
  
  <div class="col-md-12 section-heading">
    <h1>Organizers</h1>
    
  </div>
  

  

  
  

  

  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/beheshteh-tolouei-rakhshan/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/beheshteh-tolouei-rakhshan/avatar_hu79c21157058a600113a3241216d75ea3_527311_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/beheshteh-tolouei-rakhshan/">Beheshteh Tolouei Rakhshan</a></h2>
      
      
      
      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/farzaneh-heidari/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/farzaneh-heidari/avatar_hu7fba9da1679ce4a56c592454604cb9c1_94192_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/farzaneh-heidari/">Farzaneh Heidari</a></h2>
      
      
      
      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/michael-rizvi-martel/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/michael-rizvi-martel/avatar_hu2cab89305254775c75734c9d86736ddb_545100_270x270_fill_lanczos_center_3.png" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/michael-rizvi-martel/">Michael Rizvi-Martel</a></h2>
      
      
      
      
    </div>
  </div>
  
  
</div>


    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="features" class="home-section wg-blank  "  >
   <div class="home-section-bg " >
     
   </div>
    <div class="container">

    
      <div class="row  justify-content-center">
      
    

      



  <div class="col-12 col-lg-8">
    
  </div>



    
      </div>
    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="try" class="home-section wg-blank  "  >
   <div class="home-section-bg " >
     
   </div>
    <div class="container">

    
      <div class="row  justify-content-center">
      
    

      



  <div class="col-12 col-lg-8">
    
  </div>



    
      </div>
    

    </div>
  </section>







  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  

  

  

  
  






  
  <p class="powered-by copyright-license-text">
    © 2024 Me. This work is licensed under {license}
  </p>
  




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js"></script>




  

  
  

  






























<script id="page-data" type="application/json">{"use_headroom":false}</script>












<script src="/en/js/wowchemy.min.eaf8884a8347f16d217bda049c9a6a3d.js"></script>























</body>
</html>
